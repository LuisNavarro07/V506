mutate(percent_vote = candidatevotes/totalvotes) %>%
# only keep year, state, county_fips, county_name, party, candidatevotes and total votes
select(year, county_fips, party, percent_vote) %>%
# get variable that determines who got the majority of the vote
# remember the variable is still grouped by county and year
mutate(majority_vote = ifelse(percent_vote == max(percent_vote), party, NA)) %>%
# keep only the observation from the majority votes
filter(is.na(majority_vote)==FALSE) %>%
# create a republican dummy variable
mutate(republican = ifelse(majority_vote == "REPUBLICAN", 1,0)) %>%
# keep only the variables we want
select(year, county_fips, percent_vote, republican) %>%
ungroup()
election_data_clean %>%
slice_sample(n = 10)
gdp_data <-rio::import(here(lab6_path, "CAGDP9__ALL_AREAS_2017_2022.csv"))
# clean the gdp data
gdp_clean <- gdp_data %>%
# keep only totals by county
filter(Description == "All industry total ") %>%
# drop state totals
filter(GeoFIPS != 0) %>%
# drop irrelevant variables
select(-c(Region, LineCode, IndustryClassification, Unit, Description, TableName)) %>%
# data reshape long
gather(key = "year", value = "real_gdp", -c(GeoFIPS, GeoName)) %>%
# keep only observations from 2019
filter(year == 2019) %>%
# rename fips variable
rename(county_fips = GEOFIPS)
gdp_data %>%
# keep only totals by county
filter(Description == "All industry total ") %>%
# drop state totals
filter(GeoFIPS != 0)
gdp_data %>%
# keep only totals by county
filter(Description == "All industry total ") %>%
# drop state totals
filter(GeoFIPS != 0)
# clean the gdp data
gdp_clean <- gdp_data %>%
# keep only totals by county
filter(Description == "All industry total ") %>%
# drop national total
filter(GeoFIPS != 0) %>%
# drop irrelevant variables
select(-c(Region, LineCode, IndustryClassification, Unit, Description, TableName)) %>%
# data reshape long
gather(key = "year", value = "real_gdp", -c(GeoFIPS, GeoName)) %>%
# keep only observations from 2019
filter(year == 2019) %>%
# rename fips variable
rename(county_fips = GEOFIPS)
gdp_data %>%
# keep only totals by county
filter(Description == "All industry total ") %>%
# drop national total
filter(GeoFIPS != 0) %>%
# drop irrelevant variables
select(-c(Region, LineCode, IndustryClassification, Unit, Description, TableName)) %>%
# data reshape long
gather(key = "year", value = "real_gdp", -c(GeoFIPS, GeoName)) %>%
# keep only observations from 2019
filter(year == 2019)
# clean the gdp data
gdp_clean <- gdp_data %>%
# keep only totals by county
filter(Description == "All industry total ") %>%
# drop national total
filter(GeoFIPS != 0) %>%
# drop irrelevant variables
select(-c(Region, LineCode, IndustryClassification, Unit, Description, TableName)) %>%
# data reshape long
gather(key = "year", value = "real_gdp", -c(GeoFIPS, GeoName)) %>%
# keep only observations from 2019
filter(year == 2019) %>%
# rename fips variable
rename(county_fips = Geo_FIPS)
# clean the gdp data
gdp_clean <- gdp_data %>%
# keep only totals by county
filter(Description == "All industry total ") %>%
# drop national total
filter(GeoFIPS != 0) %>%
# drop irrelevant variables
select(-c(Region, LineCode, IndustryClassification, Unit, Description, TableName)) %>%
# data reshape long
gather(key = "year", value = "real_gdp", -c(GeoFIPS, GeoName)) %>%
# keep only observations from 2019
filter(year == 2019) %>%
# rename fips variable
rename(county_fips = GeoFIPS)
gdp_clean
#-------------------------------------------------------------------------------------------------------
# Project: Federal Assistance and Municipal Borrowing: Unpacking the effects of the CARES Act on Government Liquidity Management
# Script: Makefile
# Update: May 2024
# Code by: Luis Navarro
#-------------------------------------------------------------------------------------------------------
# This script runs all the code to do the analysis presented on the paper
#-------------------------------------------------------------------------------------------------------
# Clean the environment
rm(list = ls())
library(pacman)
p_load(rio, tidyverse, data.table,
lubridate, ggthemes, here,
tigris, tidycensus,
sf, scales, lsr,
fixest, cowplot,
rdrobust, rddensity, rddtools,
insight, ggstatsplot,
modelsummary, xtable)
# Set file paths
path_ipreo <- "//Users/luisenriquenavarro/Library/CloudStorage/OneDrive-IndianaUniversity/Data/ipreo/"
path = here::here('/Users/luisenriquenavarro/Library/CloudStorage/OneDrive-IndianaUniversity/Research/Cares Act/replication_package')
ac = here(path,'analysis','code')
ai = here(path,'analysis','input')
ao = here(path,'analysis','output')
bc = here(path,'build','code')
bi = here(path,'build','input')
bt = here(path,'build','temp')
# Clean the environment
rm(list=ls())
# Clean the environment
rm(list=ls())
setwd("/Users/luisenriquenavarro/Library/CloudStorage/OneDrive-IndianaUniversity/V506/Lab 5")
library(pacman)
p_load(tidyverse, rmarkdown, dplyr, ggplot2, descriptr)
# Clean the environment
rm(list=ls())
# Set your working directory
setwd("/Users/luisenriquenavarro/Library/CloudStorage/OneDrive-IndianaUniversity/V506/Fall24/Lab5")
# Packages Required for the session
library(pacman)
p_load(dplyr, ggplot2, rmarkdown, rio, here)
gdp_states <- read.csv(file = "gdp_states.csv", header = TRUE) %>% tibble()
head(gdp_states)
View(gdp_states)
gdp_wide <- gdp_states
gdp_long <- gdp_states %>%
gather(key = "year", value = "real_gdp", -c(GeoName,GeoFips))
View(gdp_long)
View(gdp_wide)
gdp_states
gdp_states %>%
gather(key = "year", value = "real_gdp", -c(GeoName,GeoFips))
gdp_states %>%
gather(key = "year", value = "real_gdp", -c(GeoName,GeoFips)) %>%
## Intermediate Step to clean year variable
mutate(year = substr(year,2,nchar(year)) %>% as.double())
gdp_long <- gdp_states %>%
gather(key = "year", value = "real_gdp", -c(GeoName,GeoFips)) %>%
## Intermediate Step to clean year variable
mutate(year = substr(year,2,nchar(year)) %>% as.double()) %>%
# Keep only states data
filter(GeoFips >0 &GeoFips < 90000) %>%
# Rename variables
rename(state = GeoName)
gdp_long
gdp_long
gdp_wide <- gdp_long %>%
spread(key = year, # name of the variable that will be columns
value = real_gdp)
gdp_wide
states_pop_long <- rio::import(file = "state_population_long.csv", header = TRUE) %>% tibble() %>% select(-V1)
head(states_pop_long)
View(states_pop_long)
gdp_long2020 <- gdp_long %>%
filter(year >= 2020)
gdp_long2020
states_pop_long
gdp_long2020
states_pop_long
gdp_long2020
states_data <- gdp_long2020 %>%
# Merge population data
left_join(states_pop_long,
by = c("state", "year"), #merge by state and year
relationship = "one-to-one")
states_data
states_data <- states_data %>%
# multiply by one million, to express everything in US dollars.
mutate(gdp_per_capita = 1000000*real_gdp/population)
states_data %>% head()
View(states_data)
starwars <- dplyr::starwars
starwars %>% pull(mass)
mean(mass_sw, na.rm = TRUE)
mass_sw <- starwars %>% pull(mass)
mean(mass_sw, na.rm = TRUE)
starwars
starwars
starwars %>%
# use summarize to compute mean, sd and number of obs
summarize(mean = mean(mass, na.rm = TRUE),
sd = sd(mass, na.rm = TRUE),
obs = n())
starwars %>%
# use summarize to compute mean, sd and number of obs
summarize(mean = mean(mass, na.rm = TRUE),
sd = sd(mass, na.rm = TRUE),
obs = n()) %>%
# compute manually the standard error, margin of error, and confidence intervals
mutate(
std.error = sd/sqrt(obs),
margin.error = std.error*qnorm(0.975),
conf.low = mean - margin.error,
conf.upp = mean + margin.error
) %>%
# create a sentence that has the confidence interval as a character
mutate(
conf.int = paste("(",round(conf.low,4),",", round(conf.upp,4),")", sep = '')
)
# Run the Test with R command
mu_null <- 50
test <- t.test(starwars$mass, mu=mu_null)
degfree <- test$parameter
test
# Compute the Test Statistic
t_stat <- (mass_mean - mu_null)/mass_se
#Define the Statistics
mass_mean <-mean(mass_sw, na.rm = TRUE)
mass_sd <-sd(mass_sw, na.rm = TRUE)
mass_obs <- mass_sw %>% na.omit() %>% length()
mass_se <- mass_sd / sqrt(mass_obs)
# Compute the Margin Error
margin_error <- mass_se*upper_value
test<-t.test(mass_sw, mu = 0)
test$conf.int
test$conf.int
margin_error <- mass_se*qnorm(0.975)
#Define the Statistics
mass_mean <-mean(mass_sw, na.rm = TRUE)
mass_sd <-sd(mass_sw, na.rm = TRUE)
mass_obs <- mass_sw %>% na.omit() %>% length()
mass_se <- mass_sd / sqrt(mass_obs)
# Compute the Margin Error
margin_error <- mass_se*qnorm(0.975)
# Compute the Confidence Interval
ci <- c(mass_mean - margin_error, mass_mean + margin_error)
ci <- paste("(",as.character(round(ci[1],4)),",",as.character(round(ci[2],4)), ")", sep="")
# Display Results
data.frame(mean = mass_mean, se = mass_se, confint = ci) %>% t()
# Run the Test with R command
mu_null <- 50
test <- t.test(starwars$mass, mu=mu_null)
degfree <- test$parameter
test
# Compute the Test Statistic
t_stat <- (mass_mean - mu_null)/mass_se
# Compute the distribution of test-stastic under the null
t_dist_null <- data.frame(x = rt(n=1000, df =degfree)) %>% tibble()
## Ggplot
t_dist_plot <- ggplot(t_dist_null, aes(x = x)) +
geom_density(color = "darkblue", fill = "lightblue", alpha = 0.3) +
geom_vline(xintercept = t_stat, color = "red", linetype = "dashed")+
theme_bw() +
labs(title = "Distribution of the Sampling Average", x = "X Bar",y = "Density")
t_dist_plot
# Run the Test with R command
mu_null <- 100
test <- t.test(starwars$mass, mu=mu_null)
degfree <- test$parameter
test
# Compute the Test Statistic
t_stat <- (mass_mean - mu_null)/mass_se
# Compute the distribution of test-stastic under the null
t_dist_null <- data.frame(x = rt(n=1000, df =degfree)) %>% tibble()
## Ggplot
t_dist_plot <- ggplot(t_dist_null, aes(x = x)) +
geom_density(color = "darkblue", fill = "lightblue", alpha = 0.3) +
geom_vline(xintercept = t_stat, color = "red", linetype = "dashed")+
theme_bw() +
labs(title = "Distribution of the Sampling Average", x = "X Bar",y = "Density")
t_dist_plot
# simulate data
obs = 100
simulated_normal_data <- data.frame(id = 1:obs,
x = rnorm(obs, mean = 0, sd = 1),
y = rnorm(obs, mean = 0.5, sd = 1))
# visualize data
simulated_normal_data %>%
# reshape data long to use exploit ggplot grammar
gather(key = "variable", value = "value", -id) %>%
ggplot(mapping = aes(x = value, color = variable, fill = variable)) +
# geom_density allows you to draw the kernel density of the simulated data
geom_density(alpha = 0.5) +
labs(x = "Values of x,y", y = "density",
title = "Distribution of Normal Simulated Data") +
scale_x_continuous(breaks = -3:4) +
theme_classic()
vector_x <- simulated_normal_data$x
vector_y <- simulated_normal_data$y
vector_x
vector_y
two_sample_test <- t.test(vector_x, vector_y)
two_sample_test
# visual representation of the two sample t-test
degfree = two_sample_test$parameter
t_stat = two_sample_test$statistic
# Compute the distribution of test-stastic under the null
t_dist_null <- data.frame(x = rt(n=1000, df =degfree)) %>% tibble()
## Ggplot
t_dist_plot <- ggplot(t_dist_null, aes(x = x)) +
geom_density(color = "darkblue", fill = "lightblue", alpha = 0.3) +
geom_vline(xintercept = t_stat, color = "red", linetype = "dashed")+
theme_bw() +
labs(title = "Distribution of the T-stat under the Null", x = "t-stat distribution",y = "Density")
t_dist_plot
# simulate data
obs = 100
simulated_normal_data <- data.frame(id = 1:obs,
x = rnorm(obs, mean = 0, sd = 1),
y = rnorm(obs, mean = 1, sd = 1))
# visualize data
simulated_normal_data %>%
# reshape data long to use exploit ggplot grammar
gather(key = "variable", value = "value", -id) %>%
ggplot(mapping = aes(x = value, color = variable, fill = variable)) +
# geom_density allows you to draw the kernel density of the simulated data
geom_density(alpha = 0.5) +
labs(x = "Values of x,y", y = "density",
title = "Distribution of Normal Simulated Data") +
scale_x_continuous(breaks = -3:4) +
theme_classic()
vector_x <- simulated_normal_data$x
vector_y <- simulated_normal_data$y
two_sample_test <- t.test(vector_x, vector_y)
two_sample_test
# visual representation of the two sample t-test
degfree = two_sample_test$parameter
t_stat = two_sample_test$statistic
# Compute the distribution of test-stastic under the null
t_dist_null <- data.frame(x = rt(n=1000, df =degfree)) %>% tibble()
## Ggplot
t_dist_plot <- ggplot(t_dist_null, aes(x = x)) +
geom_density(color = "darkblue", fill = "lightblue", alpha = 0.3) +
geom_vline(xintercept = t_stat, color = "red", linetype = "dashed")+
theme_bw() +
labs(title = "Distribution of the T-stat under the Null", x = "t-stat distribution",y = "Density")
t_dist_plot
# simulate data
obs = 100
simulated_normal_data <- data.frame(id = 1:obs,
x = rnorm(obs, mean = 0, sd = 1),
y = rnorm(obs, mean = 0.1, sd = 1))
# visualize data
simulated_normal_data %>%
# reshape data long to use exploit ggplot grammar
gather(key = "variable", value = "value", -id) %>%
ggplot(mapping = aes(x = value, color = variable, fill = variable)) +
# geom_density allows you to draw the kernel density of the simulated data
geom_density(alpha = 0.5) +
labs(x = "Values of x,y", y = "density",
title = "Distribution of Normal Simulated Data") +
scale_x_continuous(breaks = -3:4) +
theme_classic()
vector_x <- simulated_normal_data$x
vector_y <- simulated_normal_data$y
two_sample_test <- t.test(vector_x, vector_y)
two_sample_test
# simulate data
obs = 100
simulated_normal_data <- data.frame(id = 1:obs,
x = rnorm(obs, mean = 0, sd = 1),
y = rnorm(obs, mean = 0, sd = 1))
# visualize data
simulated_normal_data %>%
# reshape data long to use exploit ggplot grammar
gather(key = "variable", value = "value", -id) %>%
ggplot(mapping = aes(x = value, color = variable, fill = variable)) +
# geom_density allows you to draw the kernel density of the simulated data
geom_density(alpha = 0.5) +
labs(x = "Values of x,y", y = "density",
title = "Distribution of Normal Simulated Data") +
scale_x_continuous(breaks = -3:4) +
theme_classic()
vector_x <- simulated_normal_data$x
vector_y <- simulated_normal_data$y
two_sample_test <- t.test(vector_x, vector_y)
two_sample_test
# Clean the environment
rm(list=ls())
# Set your working directory
setwd("/Users/luisenriquenavarro/Library/CloudStorage/OneDrive-IndianaUniversity/V506/Fall24/Lab4")
# Packages Required for the session
library(pacman)
p_load(dplyr, ggplot2, rmarkdown, rio, here)
line_function <- function(x){
y = 1 + 2*x
return(y)
}
View(line_function)
x1 <- 5
x1 <- 5
y1 <- line_function(x1)
print(c(x1,y1))
y1
seq_x <- c(1:100)
seq_y <- line_function(seq_x)
seq_x
seq_y
data_line <- data.frame(x = seq_x, y = seq_y)
data_line
data_line <- data.frame(x = seq_x, y = seq_y)
data_line %>% head(n=10)
line_function1 <- function(x,b,m){
y = b + m*x
return(y)
}
data_line2 <- data.frame(x = seq_x,
y = line_function1(x = seq_x, b = 10, m = -2))
data_line2 %>% head(n=5L)
ggplot(data = data_line2, mapping = aes(x = x,y = y)) +
geom_line(col="red")+ theme_bw()
data_line2 <- data.frame(x = seq_x,
y = line_function1(x = seq_x, b = 50, m = 1))
data_line2 %>% head(n=5L)
ggplot(data = data_line2, mapping = aes(x = x,y = y)) +
geom_line(col="red")+ theme_bw()
?rnorm
create_random_normal <- function(samplesize, avg, std) {
set.seed(1234)
# Take a sample of n observations from a normal distribution with mean and sd
random_sample <- rnorm(n = n, mean = mean, sd = sd)
# Obtain the density of the sample we just simulated
normal_density <- dnorm(random_sample, mean = mean, sd = sd)
# Save the output in a data frame
df <- tibble(id = seq(1:n),
values = random_sample,
density = normal_density)
return(df)
}
sample1 <- create_random_normal(n = 1000,
mean = 1,
sd = 2)
create_random_normal <- function(n, mean, sd) {
set.seed(1234)
# Take a sample of n observations from a normal distribution with mean and sd
random_sample <- rnorm(n = n, mean = mean, sd = sd)
# Obtain the density of the sample we just simulated
normal_density <- dnorm(random_sample, mean = mean, sd = sd)
# Save the output in a data frame
df <- tibble(id = seq(1:n),
values = random_sample,
density = normal_density)
return(df)
}
sample1 <- create_random_normal(n = 1000,
mean = 1,
sd = 2)
sample1 %>% head()
sample1
fastfood <- rio::import(file = "fastfood.csv", header = TRUE) %>%
# store as tibble data frame
tibble()
# Show data: Slice_sample takes a random sample of size n
fastfood %>%
slice_sample(n = 5)
View(fastfood)
descriptive_table <- function(x){
# Compute the Descriptive Statistics using base R functions
x_count <- length(x)
x_miss <- sum(is.na(x))/length(x)
x_mean <- mean(x, na.rm = TRUE)
x_median <- median(x, na.rm = TRUE)
x_sd <- sd(x, na.rm = TRUE)
# Save output as data frame (tibble object)
table <- tibble(obs = x_count,
miss = x_miss, mean = x_mean,
median = x_median, sd = x_sd)
# Return Table
return(table)
}
descriptive_table
calories <- fastfood$calories
calories
descriptive_table(calories)
is.na(calories)
is.na(calories) %>% sum()
# Example:
calories <- fastfood$calories
carbs <- fastfood$total_carb
total_fat <- fastfood$total_fat
# Create a list of Variables
variables <- list(calories, carbs, total_fat)
variables
variables_descriptive_tables <- lapply(variables, descriptive_table)
variables_descriptive_tables
variables_descriptive_tables[[1]]
variables_descriptive_tables[[1]] %>% mutate(varname = "Calories")
bind_rows(variables_descriptive_tables[[1]] %>% mutate(varname = "Calories"),
variables_descriptive_tables[[2]] %>% mutate(varname = "Carbs"),
variables_descriptive_tables[[3]] %>% mutate(varname = "Total Fat")) %>%
# Relocate allows you to change the order of the variables in the data frame
relocate(varname)
descriptive_stats_calories <- fastfood %>%
summarize(Mean = mean(calories, na.rm = TRUE),
SD = sd(calories, na.rm = TRUE),
Min = min(calories, na.rm = TRUE),
Percent25 = quantile(calories, probs = 0.25),
Median = quantile(calories, probs = 0.50),
Percent75 = quantile(calories, probs = 0.75),
Max = max(calories, na.rm = TRUE),
Obs = n())
# Show the Table
descriptive_stats_calories
descriptive_stats_calories %>%
mutate(Variable = "Calories") %>%
# Function Relocate changes the ordering of the variables
relocate(Variable, Obs, Mean, SD)
